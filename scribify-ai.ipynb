{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40b474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install google-generativeai PyMuPDF sentence-transformers faiss-cpu opencv-python reportlab pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "988be8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pheonix/Documents/Hackathons/VOID-V!/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, glob\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "\n",
    "# RAG / Embeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "\n",
    "# PDF reports\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import A4, inch\n",
    "\n",
    "# Gemini\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4bfb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Configure Gemini\n",
    "# -------------------------\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCXUQ-6FuRqBQQwc43IEq49dvoHv9usnZ8\"  # <-- REPLACE ME\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "GEMINI_MODEL = genai.GenerativeModel(\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf083a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Utility: PDF helpers\n",
    "# -------------------------\n",
    "def pdf_to_images(pdf_path: str, out_dir: str = \"data/tmp_pages\", dpi: int = 150) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert all pages of a PDF into images and return image file paths.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    im_paths = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    base = Path(pdf_path).stem\n",
    "    for i, page in enumerate(doc, start=1):\n",
    "        pix = page.get_pixmap(dpi=dpi)\n",
    "        ipath = os.path.join(out_dir, f\"{base}_page{i}.jpg\")\n",
    "        pix.save(ipath)\n",
    "        im_paths.append(ipath)\n",
    "    return im_paths\n",
    "\n",
    "def read_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts plaintext from a PDF (for question paper / subject book).\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for p in doc:\n",
    "            text.append(p.get_text())\n",
    "    return \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e863dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Agent 1 — Document Intelligence (Gemini Vision, no OCR)\n",
    "# Extracts per-page text + diagram descriptions as JSON.\n",
    "# ============================================================\n",
    "def extract_student_pages_json(student_pdf: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each page of the student's PDF:\n",
    "      - Convert to image\n",
    "      - Ask Gemini to extract all text + describe diagrams\n",
    "      - Return structured list[ {page, text, diagrams} ]\n",
    "    \"\"\"\n",
    "    page_images = pdf_to_images(student_pdf)\n",
    "    pages = []\n",
    "    for i, img_path in enumerate(page_images, start=1):\n",
    "        img = Image.open(img_path)\n",
    "        prompt = (\n",
    "            \"Extract ALL meaningful information from this answer sheet page.\\n\"\n",
    "            \"Return JSON with keys: page, text, diagrams.\\n\"\n",
    "            \"text: the handwritten/printed content verbatim (best effort).\\n\"\n",
    "            \"diagrams: short descriptions of any diagrams/equations tables.\\n\"\n",
    "            \"If nothing present for a key, use empty string.\"\n",
    "        )\n",
    "        resp = GEMINI_MODEL.generate_content([prompt, img])\n",
    "        raw = resp.text.strip()\n",
    "        # Gemini usually returns JSON-like; be defensive:\n",
    "        try:\n",
    "            # Try JSON parse; if it fails, wrap it into a JSON\n",
    "            data = json.loads(raw)\n",
    "            if isinstance(data, dict):\n",
    "                data[\"page\"] = data.get(\"page\", i)\n",
    "                pages.append(data)\n",
    "            elif isinstance(data, list):\n",
    "                # If a list, add all but ensure page is present\n",
    "                for d in data:\n",
    "                    if isinstance(d, dict):\n",
    "                        d[\"page\"] = d.get(\"page\", i)\n",
    "                        pages.append(d)\n",
    "            else:\n",
    "                pages.append({\"page\": i, \"text\": raw, \"diagrams\": \"\"})\n",
    "        except Exception:\n",
    "            pages.append({\"page\": i, \"text\": raw, \"diagrams\": \"\"})\n",
    "    return pages\n",
    "\n",
    "def merge_student_pages(pages_json: List[Dict[str, Any]]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Merge all page texts and diagram descriptions into a single student-level blob.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    diags = []\n",
    "    for p in pages_json:\n",
    "        t = p.get(\"text\", \"\")\n",
    "        d = p.get(\"diagrams\", \"\")\n",
    "        if t: texts.append(f\"[Page {p.get('page', '?')}] {t}\")\n",
    "        if d: diags.append(f\"[Page {p.get('page', '?')}] {d}\")\n",
    "    return {\n",
    "        \"text\": \"\\n\".join(texts).strip(),\n",
    "        \"diagrams\": \"\\n\".join(diags).strip()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77f4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Agent 2 — Question & Structure Mapper\n",
    "# Parse Question Paper → parts, qno, marks, question.\n",
    "# Map student content → question numbers.\n",
    "# ============================================================\n",
    "def parse_question_paper_to_schema(qpaper_pdf: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns a list of items: {part, qno, marks, question}\n",
    "    \"\"\"\n",
    "    qp_text = read_pdf_text(qpaper_pdf)\n",
    "    prompt = (\n",
    "        \"Parse the following exam question paper text. \"\n",
    "        \"Extract a JSON array where each element has keys: \"\n",
    "        'part (e.g., \"Part A\"), qno (integer), marks (integer), question (string). '\n",
    "        \"If marks per part are specified in headings (e.g., 'Part A (2 marks each)'), \"\n",
    "        \"apply that to each question in that part. Ensure clean integers.\\n\\n\"\n",
    "        f\"Question Paper Text:\\n{qp_text}\"\n",
    "    )\n",
    "    resp = GEMINI_MODEL.generate_content(prompt)\n",
    "    raw = resp.text.strip()\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "        # minimal sanity check & cleanup\n",
    "        cleaned = []\n",
    "        for d in data:\n",
    "            cleaned.append({\n",
    "                \"part\": str(d.get(\"part\", \"\")).strip() or \"Unknown\",\n",
    "                \"qno\": int(d.get(\"qno\", 0)),\n",
    "                \"marks\": int(d.get(\"marks\", 0)),\n",
    "                \"question\": str(d.get(\"question\", \"\")).strip()\n",
    "            })\n",
    "        # sort by qno\n",
    "        cleaned.sort(key=lambda x: x[\"qno\"])\n",
    "        return cleaned\n",
    "    except Exception:\n",
    "        # Fallback: naive split by Q\\d\n",
    "        lines = [ln.strip() for ln in qp_text.splitlines() if ln.strip()]\n",
    "        items = []\n",
    "        part = \"Unknown\"\n",
    "        default_marks = 0\n",
    "        for ln in lines:\n",
    "            # detect part + marks\n",
    "            mpart = re.search(r\"(Part\\s*[A-Z])\\s*\\((\\d+)\\s*marks\", ln, flags=re.I)\n",
    "            if mpart:\n",
    "                part = mpart.group(1).title()\n",
    "                default_marks = int(mpart.group(2))\n",
    "                continue\n",
    "            mpart2 = re.search(r\"(Part\\s*[A-Z])\", ln, flags=re.I)\n",
    "            if mpart2:\n",
    "                part = mpart2.group(1).title()\n",
    "                continue\n",
    "            mq = re.match(r\"(\\d+)[\\).:-]\\s*(.+)\", ln)\n",
    "            if mq:\n",
    "                qno = int(mq.group(1))\n",
    "                qtext = mq.group(2).strip()\n",
    "                items.append({\"part\": part, \"qno\": qno, \"marks\": default_marks or 1, \"question\": qtext})\n",
    "        return items\n",
    "\n",
    "def map_student_answers_to_questions(student_blob: Dict[str, str],\n",
    "                                     schema: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use Gemini to align student's merged text to each question number.\n",
    "    Returns list of {part,qno,marks,question,student_answer,student_diagrams}.\n",
    "    \"\"\"\n",
    "    # Create a compact schema string\n",
    "    schema_str = \"\\n\".join([f\"{it['part']} | Q{it['qno']} | {it['marks']} marks | {it['question']}\"\n",
    "                            for it in schema])\n",
    "\n",
    "    prompt = (\n",
    "        \"You are aligning an answer script to a question paper. \"\n",
    "        \"Given the list of questions and the student's entire answer text & diagram notes, \"\n",
    "        \"split and map the student's content to each question number. \"\n",
    "        \"Return a JSON array where each element has: \"\n",
    "        '{\"part\": \"...\", \"qno\": ..., \"marks\": ..., \"question\": \"...\", '\n",
    "        '\"student_answer\": \"...\", \"student_diagrams\": \"...\"} '\n",
    "        \"If a question is unanswered, set student_answer to empty string.\"\n",
    "        \"\\n\\nQuestions:\\n\"\n",
    "        f\"{schema_str}\\n\\n\"\n",
    "        \"Student Answer Text:\\n\"\n",
    "        f\"{student_blob.get('text','')}\\n\\n\"\n",
    "        \"Student Diagram Notes:\\n\"\n",
    "        f\"{student_blob.get('diagrams','')}\"\n",
    "    )\n",
    "\n",
    "    resp = GEMINI_MODEL.generate_content(prompt)\n",
    "    raw = resp.text.strip()\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "        # ensure all qnos present; if not, backfill with defaults\n",
    "        mapped = []\n",
    "        for it in data:\n",
    "            mapped.append({\n",
    "                \"part\": it.get(\"part\",\"Unknown\"),\n",
    "                \"qno\": int(it.get(\"qno\", 0)),\n",
    "                \"marks\": int(it.get(\"marks\", 0)),\n",
    "                \"question\": it.get(\"question\",\"\").strip(),\n",
    "                \"student_answer\": it.get(\"student_answer\",\"\").strip(),\n",
    "                \"student_diagrams\": it.get(\"student_diagrams\",\"\").strip()\n",
    "            })\n",
    "        # keep only those present in schema qnos\n",
    "        valid_qnos = {s[\"qno\"] for s in schema}\n",
    "        mapped = [m for m in mapped if m[\"qno\"] in valid_qnos]\n",
    "        # fill in missing qnos with blanks if needed\n",
    "        present = {m[\"qno\"] for m in mapped}\n",
    "        for s in schema:\n",
    "            if s[\"qno\"] not in present:\n",
    "                mapped.append({\n",
    "                    \"part\": s[\"part\"], \"qno\": s[\"qno\"], \"marks\": s[\"marks\"],\n",
    "                    \"question\": s[\"question\"], \"student_answer\":\"\", \"student_diagrams\":\"\"\n",
    "                })\n",
    "        mapped.sort(key=lambda x: x[\"qno\"])\n",
    "        return mapped\n",
    "    except Exception:\n",
    "        # Fallback: naive all text for all questions (hackathon safe default)\n",
    "        mapped = []\n",
    "        for s in schema:\n",
    "            mapped.append({\n",
    "                \"part\": s[\"part\"], \"qno\": s[\"qno\"], \"marks\": s[\"marks\"], \"question\": s[\"question\"],\n",
    "                \"student_answer\": student_blob.get(\"text\",\"\"), \"student_diagrams\": student_blob.get(\"diagrams\",\"\")\n",
    "            })\n",
    "        return mapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e2d252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Agent 3 — Concept Retriever (RAG) + Grading Brain (Gemini)\n",
    "# Build FAISS index over subject book; grade per mapped Q.\n",
    "# ============================================================\n",
    "EMBED = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def chunk_text(text: str, words_per_chunk: int = 250) -> List[str]:\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+words_per_chunk]) for i in range(0, len(words), words_per_chunk)]\n",
    "\n",
    "def build_rag_index(subject_book_pdf: str):\n",
    "    sb_text = read_pdf_text(subject_book_pdf)\n",
    "    chunks = chunk_text(sb_text, 250)\n",
    "    if not chunks:\n",
    "        chunks = [sb_text]\n",
    "    vecs = EMBED.encode(chunks)\n",
    "    index = faiss.IndexFlatL2(vecs.shape[1])\n",
    "    index.add(np.array(vecs))\n",
    "    return chunks, index\n",
    "\n",
    "def retrieve_context(query: str, chunks: List[str], index, k: int = 3) -> str:\n",
    "    qv = EMBED.encode([query])\n",
    "    D, I = index.search(np.array(qv), k)\n",
    "    selected = [chunks[i] for i in I[0]]\n",
    "    return \"\\n\".join(selected)\n",
    "\n",
    "def grade_one_question(item: Dict[str, Any], context: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    item: {part,qno,marks,question,student_answer,student_diagrams}\n",
    "    Returns: { ... , score, feedback }\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an expert teacher. Grade the student's answer using the reference context from the textbook. \"\n",
    "        \"Be concise and fair.\\n\\n\"\n",
    "        f\"Question (Part {item['part']}): Q{item['qno']} ({item['marks']} marks)\\n\"\n",
    "        f\"{item['question']}\\n\\n\"\n",
    "        \"Textbook Context (relevant excerpts):\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Student Answer:\\n\"\n",
    "        f\"{item.get('student_answer','')}\\n\\n\"\n",
    "        \"Student Diagram Description:\\n\"\n",
    "        f\"{item.get('student_diagrams','')}\\n\\n\"\n",
    "        f\"Instructions:\\n- Give a numeric score out of {item['marks']}.\\n\"\n",
    "        \"- Explain briefly: what is correct, what is missing, and how to improve.\\n\"\n",
    "        \"- Respond in this JSON format strictly:\\n\"\n",
    "        '{\"score\": <number>, \"feedback\": \"<one short paragraph>\"}'\n",
    "    )\n",
    "    resp = GEMINI_MODEL.generate_content(prompt)\n",
    "    raw = resp.text.strip()\n",
    "    # Try to parse JSON; if fail, try to extract number + feedback heuristically\n",
    "    score = 0.0\n",
    "    feedback = raw\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "        score = float(data.get(\"score\", 0))\n",
    "        feedback = str(data.get(\"feedback\",\"\")).strip() or raw\n",
    "    except Exception:\n",
    "        # Heuristic score grab\n",
    "        ms = re.search(r\"(\\d+(\\.\\d+)?)\", raw)\n",
    "        if ms: score = float(ms.group(1))\n",
    "    # Clamp score between 0 and marks\n",
    "    score = max(0.0, min(score, float(item[\"marks\"])))\n",
    "    out = dict(item)\n",
    "    out.update({\"score\": score, \"feedback\": feedback})\n",
    "    return out\n",
    "\n",
    "def grade_all(mapped_items: List[Dict[str, Any]], chunks: List[str], index) -> List[Dict[str, Any]]:\n",
    "    graded = []\n",
    "    for it in mapped_items:\n",
    "        # retrieve context using the actual question text\n",
    "        ctx = retrieve_context(it[\"question\"], chunks, index, k=3)\n",
    "        graded.append(grade_one_question(it, ctx))\n",
    "    return graded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39a8eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Agent 4 — Report Generator (per-student PDF)\n",
    "# ============================================================\n",
    "def generate_student_report(student_name: str, graded_items: List[Dict[str, Any]], out_dir=\"reports\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    pdf_path = os.path.join(out_dir, f\"{student_name}.pdf\")\n",
    "    pdf = canvas.Canvas(pdf_path, pagesize=A4)\n",
    "    pdf.setTitle(f\"Scribify AI — {student_name}\")\n",
    "\n",
    "    # Header\n",
    "    pdf.setFont(\"Helvetica-Bold\", 16)\n",
    "    pdf.drawString(1*inch, 11*inch, f\"Scribify AI Report — {student_name}\")\n",
    "\n",
    "    y = 10.6*inch\n",
    "    total_scored = 0.0\n",
    "    total_marks = 0.0\n",
    "\n",
    "    # Sort by part, then qno\n",
    "    graded_items = sorted(graded_items, key=lambda x: (x[\"part\"], x[\"qno\"]))\n",
    "\n",
    "    current_part = None\n",
    "    for gi in graded_items:\n",
    "        if y < 1.5*inch:\n",
    "            pdf.showPage()\n",
    "            pdf.setFont(\"Helvetica-Bold\", 16)\n",
    "            pdf.drawString(1*inch, 11*inch, f\"Scribify AI Report — {student_name}\")\n",
    "            y = 10.6*inch\n",
    "\n",
    "        # Section per Part\n",
    "        if gi[\"part\"] != current_part:\n",
    "            current_part = gi[\"part\"]\n",
    "            pdf.setFont(\"Helvetica-Bold\", 13)\n",
    "            pdf.drawString(1*inch, y, f\"{current_part}\")\n",
    "            y -= 0.25*inch\n",
    "\n",
    "        # Question line\n",
    "        pdf.setFont(\"Helvetica-Bold\", 11)\n",
    "        pdf.drawString(1*inch, y, f\"Q{gi['qno']}  [{gi['marks']} marks] — Score: {gi['score']:.1f}\")\n",
    "        y -= 0.2*inch\n",
    "\n",
    "        # Feedback\n",
    "        pdf.setFont(\"Helvetica\", 10)\n",
    "        # Wrap feedback roughly\n",
    "        fb = gi.get(\"feedback\",\"\").replace(\"\\n\", \" \").strip()\n",
    "        wrapped = []\n",
    "        line = \"\"\n",
    "        for word in fb.split():\n",
    "            if len(line) + len(word) + 1 < 110:\n",
    "                line = f\"{line} {word}\".strip()\n",
    "            else:\n",
    "                wrapped.append(line)\n",
    "                line = word\n",
    "        if line: wrapped.append(line)\n",
    "        for w in wrapped[:8]:  # limit for page\n",
    "            pdf.drawString(1*inch, y, w)\n",
    "            y -= 0.18*inch\n",
    "        y -= 0.08*inch\n",
    "\n",
    "        total_scored += float(gi[\"score\"])\n",
    "        total_marks += float(gi[\"marks\"])\n",
    "\n",
    "    # Totals\n",
    "    if y < 1.2*inch:\n",
    "        pdf.showPage()\n",
    "        y = 10.6*inch\n",
    "    pdf.setFont(\"Helvetica-Bold\", 12)\n",
    "    pdf.drawString(1*inch, y, f\"Total: {total_scored:.1f} / {total_marks:.1f}\")\n",
    "    pdf.save()\n",
    "    return pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45eab6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Building RAG index from subject book...\n",
      "   -> Chunks in KB: 540\n",
      "🧭 Parsing question paper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761285249.857578 1396299 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Parsed 5 questions\n",
      "\n",
      "📄 Student: Dharun355\n",
      "✅ Report: reports/Dharun355.pdf\n",
      "All reports: ['reports/Dharun355.pdf']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# End-to-End Driver\n",
    "# ============================================================\n",
    "def run_scribify_pipeline(\n",
    "    question_paper_pdf: str = \"data/question_paper.pdf\",\n",
    "    subject_book_pdf: str = \"data/subject_book.pdf\",\n",
    "    students_glob: str = \"data/students/*.pdf\",\n",
    "    limit_questions: int = None  # e.g., 5 to cap during demo\n",
    "):\n",
    "    # Build RAG index for subject book once\n",
    "    print(\"🔧 Building RAG index from subject book...\")\n",
    "    chunks, index = build_rag_index(subject_book_pdf)\n",
    "    print(f\"   -> Chunks in KB: {len(chunks)}\")\n",
    "\n",
    "    # Parse question paper to schema\n",
    "    print(\"🧭 Parsing question paper...\")\n",
    "    schema = parse_question_paper_to_schema(question_paper_pdf)\n",
    "    if limit_questions:\n",
    "        schema = schema[:limit_questions]\n",
    "    print(f\"   -> Parsed {len(schema)} questions\")\n",
    "\n",
    "    # Iterate over students\n",
    "    reports = []\n",
    "    for student_pdf in glob.glob(students_glob):\n",
    "        student_name = Path(student_pdf).stem\n",
    "        print(f\"\\n📄 Student: {student_name}\")\n",
    "\n",
    "        # Agent 1: extract text + diagrams per page, then merge\n",
    "        pages_json = extract_student_pages_json(student_pdf)\n",
    "        merged = merge_student_pages(pages_json)\n",
    "\n",
    "        # Agent 2: map student's merged content to question schema\n",
    "        mapped = map_student_answers_to_questions(merged, schema)\n",
    "\n",
    "        # Agent 3: grade all mapped questions using RAG + Gemini\n",
    "        graded = grade_all(mapped, chunks, index)\n",
    "\n",
    "        # Agent 4: generate PDF report\n",
    "        pdf_path = generate_student_report(student_name, graded)\n",
    "        print(f\"✅ Report: {pdf_path}\")\n",
    "        reports.append(pdf_path)\n",
    "    return reports\n",
    "\n",
    "# ============================================================\n",
    "# RUN (uncomment to execute after you add your PDFs & API key)\n",
    "# ============================================================\n",
    "reports = run_scribify_pipeline(\n",
    "    question_paper_pdf=\"data/question_paper.pdf\",\n",
    "    subject_book_pdf=\"data/subject_book.pdf\",\n",
    "    students_glob=\"data/students/*.pdf\",\n",
    "    limit_questions= None   # set None to grade all questions\n",
    ")\n",
    "print(\"All reports:\", reports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
